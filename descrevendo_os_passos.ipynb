{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8628d9f9",
   "metadata": {},
   "source": [
    "## Desafio Indicium\n",
    "\n",
    "### Proposta\n",
    "\n",
    "Construir um pipeline de dados que extraia todos os dias de duas fontes de dados (csv e postgres), gravando os dados em um disco local e posteriormente submeter as informações para um banco de dados.\n",
    "\n",
    "Dados: São fornecidos duas fontes de dados, um banco Postgres e um arquivo CSV.\n",
    "\n",
    "As estapas devem ser isoladas umas das outras sendo capaz de executar cada uma sem executar a outra.\n",
    "\n",
    "#### Primeira etapa:\n",
    "* Gravar os dados do disco local sendo um arquivo csv para cada tabela do banco fornecido e um arquivo para o arquivo csv de entrada.\n",
    "* Será executado todos os dias, portanto a extração deve ter caminhos diferentes apontando cada dia da extração.\n",
    " \n",
    "#### Segunda etapa:\n",
    "* Carregar os arquivos os dados do sistema de arquivos local para um banco de dados.\n",
    "* Objetivo final é conseguir executar uma consulta mostrando os pedidos e seus detalhes. Os pedidos estão em uma tabela chamado orders no banco de dados postgres. Os detalhes são colocados no arquivo CSV fornecido no desafio, \n",
    " \n",
    "#### Requisitos\n",
    "* Todas as tarefas devem ser idempotentes, você deve conseguir executar todo o pipeline por um dia e o resultado deve ser sempre o mesmo\n",
    "* A etapa 2 depende de ambas as tarefas da etapa 1, portanto, você não poderá executar a etapa 2 por um dia se as tarefas da etapa 1 não forem bem-sucedidas\n",
    "* Você deve extrair todas as tabelas do banco de dados de origem, não importa que você não vá usar a maioria delas para a etapa final.\n",
    "* Você deve ser capaz de dizer claramente onde o pipeline falhou, para saber a partir de qual etapa deve executar novamente o pipeline\n",
    "* Você deve fornecer instruções claras sobre como executar todo o pipeline. Quanto mais fácil, melhor.\n",
    "* Você deve fornecer um arquivo csv ou json com o resultado da consulta final no banco de dados final.\n",
    "* Você não precisa realmente agendar o pipeline, mas deve presumir que ele será executado em dias diferentes.\n",
    "* Seu pipeline deve estar preparado para ser executado nos últimos dias, o que significa que você deve ser capaz de passar um argumento para o pipeline com um dia anterior e ele deve reprocessar os dados desse dia. Como os dados desse desafio são estáticos, a única diferença para cada dia de execução serão os caminhos de saída.\n",
    "\n",
    "link com todos os detalhes: https://github.com/techindicium/code-challenge/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9479308f",
   "metadata": {},
   "source": [
    "### Instalando o docker desktop\n",
    "\n",
    "* Acesse o link: https://www.docker.com/ escolha sua plataforma e instale o docker desktop.\n",
    "* Após concluir a instalação acesse a documentação para baixar a imagem postgres, link: https://hub.docker.com/_/postgres\n",
    "* Acesse seu terminal e execute o comando abaixo para crianção do contanier postgres:\n",
    "\n",
    "```bash\n",
    "docker run --name indicium -p 5432:5432 -e POSTGRES_USER=northwind_user -e POSTGRES_PASSWORD=thewindisblowing -e POSTGRES_DB=northwind -d postgres\n",
    "```\n",
    "\n",
    "Acesse o arquivo [**northwind.sql**](https://github.com/will-rds/desafio_pipeline_indicium/blob/main/scripts/northwind.sql) e execute o script sql dentro do banco para criar a massa de dados e tabelas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3f3650",
   "metadata": {},
   "source": [
    "### Extraindo quais tabelas existem dentro do banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7db48f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas encontradas no banco de dados:\n",
      "['us_states', 'customers', 'orders', 'employees', 'shippers', 'categories', 'products', 'suppliers', 'region', 'territories', 'employee_territories', 'customer_demographics', 'customer_customer_demo']\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "# Configurações do banco de dados PostgreSQL\n",
    "CONFIG_BD = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'northwind',\n",
    "    'user': 'northwind_user',\n",
    "    'password': 'thewindisblowing'\n",
    "}\n",
    "\n",
    "# Puxando todas as tabelas existentes no banco de dados\n",
    "def obter_nomes_tabelas(conexao):\n",
    "    nomes_tabelas = []\n",
    "    try:\n",
    "        cursor = conexao.cursor()\n",
    "        cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public';\")\n",
    "        nomes_tabelas = [row[0] for row in cursor.fetchall()]\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao recuperar os nomes das tabelas: {e}\")\n",
    "\n",
    "    return nomes_tabelas\n",
    "\n",
    "# Mostrando as tabelas encontradas no banco de dados\n",
    "def main():\n",
    "    try:\n",
    "        # Conectando ao banco de dados\n",
    "        conexao = psycopg2.connect(**CONFIG_BD)\n",
    "\n",
    "        # Recuperar os nomes das tabelas\n",
    "        nomes_tabelas = obter_nomes_tabelas(conexao)\n",
    "        print(\"Tabelas encontradas no banco de dados:\")\n",
    "        print(nomes_tabelas)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Fechar a conexão com o banco de dados\n",
    "        if conexao:\n",
    "            conexao.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f1a0e",
   "metadata": {},
   "source": [
    "### Extraindo as tabelas e salvando cada uma com a data do dia da extração em uma pasta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dcee47",
   "metadata": {},
   "source": [
    "* No diretório deste [projeto](https://github.com/will-rds/desafio_pipeline_indicium/tree/main/scripts), você irá encontrar todos arquivos com os mesmos códigos abaixo. \n",
    "* Ao executar o arquivo [ordem_execucao.py](https://github.com/will-rds/desafio_pipeline_indicium/blob/main/scripts/ordem_execucao.py) o mesmo irá chamar os demais arquivos automaticamente. \n",
    "* Lembrando que todos os arquivos devem estar no mesmo diretório.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f7d9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extração concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Ignorando mensagens do pandas\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas only supports SQLAlchemy connectable.*\")\n",
    "# Configurações do banco de dados PostgreSQL\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'northwind',\n",
    "    'user': 'northwind_user',\n",
    "    'password': 'thewindisblowing'\n",
    "}\n",
    "\n",
    "# Lista das tabelas a serem extraídas\n",
    "tabelas_extraidas = ['us_states', 'customers', 'orders', 'employees', 'shippers', 'categories', 'products', 'suppliers', 'region', 'territories', 'employee_territories', 'customer_demographics', 'customer_customer_demo']  # Adicione os nomes das tabelas aqui\n",
    "\n",
    "# Diretório base onde os arquivos CSV serão salvos (se não existir, será criado)\n",
    "DIRETORIO_BASE = os.path.join('.', 'data')\n",
    "\n",
    "def extraindo_tabelas(nome_tabela, conexao, output_dir):\n",
    "    query = f\"SELECT * FROM {nome_tabela};\"\n",
    "    df = pd.read_sql_query(query, conexao)\n",
    "    now = datetime.now()\n",
    "    date_str = now.strftime('%Y%m%d')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_filename = f\"{nome_tabela}_{date_str}.csv\"\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Conectando ao banco de dados\n",
    "        conexao = psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "        # Criar diretório com a data do dia da extração\n",
    "        date_dir = datetime.now().strftime('%Y%m%d')\n",
    "        diretorio_salvamento = os.path.join(DIRETORIO_BASE, date_dir)\n",
    "        os.makedirs(diretorio_salvamento, exist_ok=True)\n",
    "\n",
    "        # Iterar sobre as tabelas e extrair cada uma para um arquivo CSV dentro da pasta data\n",
    "        for nome_tabela in tabelas_extraidas:\n",
    "            extraindo_tabelas(nome_tabela, conexao, diretorio_salvamento)\n",
    "\n",
    "        print(\"Extração concluída com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Fechar a conexão com o banco de dados\n",
    "        if conexao:\n",
    "            conexao.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce5bba",
   "metadata": {},
   "source": [
    "### Copiando o arquivo CSV para a mesma pasta onde as tabelas se encontram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb65768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cópia do arquivo CSV concluída com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Diretório base onde os arquivos CSV serão salvos (se não existir, será criado)\n",
    "DIRETORIO_BASE = os.path.join('.', 'data')\n",
    "\n",
    "#copiando o arquivo para o diretório das demais planilhas extraidas do banco\n",
    "def main():\n",
    "    try:\n",
    "        # Criar diretório com a data do dia da extração\n",
    "        data_diretorio = datetime.now().strftime('%Y%m%d')\n",
    "        diretorio_salvamento = os.path.join(DIRETORIO_BASE, data_diretorio)\n",
    "        os.makedirs(diretorio_salvamento, exist_ok=True)\n",
    "\n",
    "        # Caminho onde o arquivo orginial csv se encontra\n",
    "        caminho_base = \"C:/indicium/order_details.csv\"\n",
    "\n",
    "        # Obter o nome do arquivo original\n",
    "        arquivo_original = os.path.basename(caminho_base)\n",
    "        \n",
    "        # Adicionar a data da extração ao nome do arquivo\n",
    "        now = datetime.now()\n",
    "        data_hoje = now.strftime('%Y%m%d')\n",
    "        novo_nome_csv = f\"{arquivo_original.split('.')[0]}_{data_hoje}.csv\"\n",
    "        destino_novo_csv = os.path.join(diretorio_salvamento, novo_nome_csv)\n",
    "\n",
    "        # Copiar o arquivo CSV para o mesmo diretório das tabelas\n",
    "        shutil.copy(caminho_base, destino_novo_csv)\n",
    "\n",
    "        print(\"Cópia do arquivo CSV concluída com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf00e7f",
   "metadata": {},
   "source": [
    "### Carregando arquivos para banco de dados.\n",
    "* Para fins didáticos foi criado um schema chamado \"datawarehouse\" no mesmo banco de dandos onde ocorreu a extração.\n",
    "* Antes de executar este bloco, crie as tabelas dentro do schema \"datawarehouse\" com o código abaixo:\n",
    "\n",
    "```sql\n",
    "-- Table: datawarehouse.orders\n",
    "\n",
    "-- DROP TABLE IF EXISTS datawarehouse.orders;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS datawarehouse.orders\n",
    "(\n",
    "    order_id smallint NOT NULL,\n",
    "    customer_id bpchar COLLATE pg_catalog.\"default\",\n",
    "    employee_id smallint,\n",
    "    order_date date,\n",
    "    required_date date,\n",
    "    shipped_date date,\n",
    "    ship_via smallint,\n",
    "    freight real,\n",
    "    ship_name character varying(40) COLLATE pg_catalog.\"default\",\n",
    "    ship_address character varying(60) COLLATE pg_catalog.\"default\",\n",
    "    ship_city character varying(15) COLLATE pg_catalog.\"default\",\n",
    "    ship_region character varying(15) COLLATE pg_catalog.\"default\",\n",
    "    ship_postal_code character varying(10) COLLATE pg_catalog.\"default\",\n",
    "    ship_country character varying(15) COLLATE pg_catalog.\"default\",\n",
    "    PRIMARY KEY (order_id)\n",
    ")\n",
    "\n",
    "TABLESPACE pg_default;\n",
    "\n",
    "ALTER TABLE IF EXISTS datawarehouse.orders\n",
    "    OWNER to northwind_user;\n",
    "\n",
    "-- Table: datawarehouse.order_details\n",
    "\n",
    "-- DROP TABLE IF EXISTS datawarehouse.order_details;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS datawarehouse.order_details\n",
    "(\n",
    "    order_id smallint NOT NULL,\n",
    "    product_id smallint,\n",
    "    unit_price real,\n",
    "    quantity smallint,\n",
    "    discount real,\n",
    "    CONSTRAINT order_details_order_id_fkey FOREIGN KEY (order_id)\n",
    "        REFERENCES datawarehouse.orders (order_id) MATCH SIMPLE\n",
    "        ON UPDATE NO ACTION\n",
    "        ON DELETE NO ACTION\n",
    ")\n",
    "\n",
    "TABLESPACE pg_default;\n",
    "\n",
    "ALTER TABLE IF EXISTS datawarehouse.order_details\n",
    "    OWNER to northwind_user;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5334ddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados na tabela orders com sucesso!\n",
      "Dados carregados na tabela order_details com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Configurações do banco de dados PostgreSQL de destino\n",
    "CONFIG_BD_DESTINO = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'northwind',\n",
    "    'user': 'northwind_user',\n",
    "    'password': 'thewindisblowing'\n",
    "}\n",
    "\n",
    "# Diretório base onde os arquivos CSV foram salvos\n",
    "DIRETORIO_BASE = os.path.join('.', 'data', datetime.now().strftime('%Y%m%d'))\n",
    "\n",
    "# Obter o dia de hoje para puxar os arquivos com a data corrente.\n",
    "data_corrente = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "# Dicionário com o mapeamento entre nome do arquivo CSV e nome da tabela\n",
    "MAPEAMENTO_ARQUIVO_TABELA = {\n",
    "    f'orders_{data_corrente}.csv': 'orders',\n",
    "    f'order_details_{data_corrente}.csv': 'order_details'\n",
    "}\n",
    "# Trucando a tabela antes de carregar novamenta para não ocorrer erro, isso pode mudar de acordo com as condições\n",
    "# do projeto, sendo incremental ou full.\n",
    "def truncar_tabelas(conexao):\n",
    "    with conexao.cursor() as cursor:\n",
    "        for nome_tabela in MAPEAMENTO_ARQUIVO_TABELA.values():\n",
    "            consulta_truncate = f\"TRUNCATE TABLE datawarehouse.{nome_tabela} CASCADE;\"\n",
    "            cursor.execute(consulta_truncate)\n",
    "        conexao.commit()\n",
    "\n",
    "# Carregando dados dos arquivos csv para o banco de dados \n",
    "def carregar_dados_no_banco():\n",
    "    conexao_destino = None\n",
    "    try:\n",
    "        # Conectar ao banco de dados de destino\n",
    "        conexao_destino = psycopg2.connect(**CONFIG_BD_DESTINO)\n",
    "\n",
    "        # Criar o esquema (schema) caso não exista\n",
    "        with conexao_destino.cursor() as cursor:\n",
    "            cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS datawarehouse;\")\n",
    "        conexao_destino.commit()\n",
    "\n",
    "        # Truncar as tabelas antes de carregar os dados\n",
    "        truncar_tabelas(conexao_destino)\n",
    "\n",
    "        for nome_arquivo, nome_tabela in MAPEAMENTO_ARQUIVO_TABELA.items():\n",
    "            # Caminho completo do arquivo CSV a ser carregado\n",
    "            caminho_arquivo_csv = os.path.join(DIRETORIO_BASE, nome_arquivo)\n",
    "\n",
    "            # Carregar o arquivo CSV em um DataFrame\n",
    "            df = pd.read_csv(caminho_arquivo_csv)\n",
    "\n",
    "            # Substituir valores \"NaN\" por \"None\" no DataFrame\n",
    "            df = df.where(pd.notna(df), None)\n",
    "\n",
    "            # Converter o DataFrame em uma lista de tuplas\n",
    "            dados = [tuple(row) for row in df.values]\n",
    "\n",
    "            # Preparar o comando SQL para inserção dos dados na tabela especificada\n",
    "            placeholders = \",\".join([\"%s\"] * len(df.columns))\n",
    "            consulta_insercao = f\"INSERT INTO datawarehouse.{nome_tabela} VALUES ({placeholders});\"\n",
    "\n",
    "            # Executar a inserção dos dados\n",
    "            with conexao_destino.cursor() as cursor:\n",
    "                cursor.executemany(consulta_insercao, dados)\n",
    "\n",
    "            conexao_destino.commit()\n",
    "\n",
    "            print(f\"Dados carregados na tabela {nome_tabela} com sucesso!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro durante o carregamento dos dados: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Fechar a conexão com o banco de dados de destino\n",
    "        if conexao_destino:\n",
    "            conexao_destino.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    carregar_dados_no_banco()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c693e0",
   "metadata": {},
   "source": [
    "### Orquestrando a ordem de execução\n",
    "* Nesta etapa vamos organizar a ordem de execução de cada scrip.\n",
    "* Caso seja encontrado algum problema no processo de extração, a carga no banco de dados será suspendida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78506875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extração concluída com sucesso!\n",
      "Cópia do arquivo CSV concluída com sucesso!\n",
      "Dados carregados na tabela orders com sucesso!\n",
      "Dados carregados na tabela order_details com sucesso!\n"
     ]
    }
   ],
   "source": [
    "import carrega_banco_postgres\n",
    "import extrai_csv\n",
    "import extrai_banco_postgres\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Extrair dados do banco de dados\n",
    "        extrai_banco_postgres.main()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na extração de dados do banco de dados: {e}\")\n",
    "        print(\"O pipeline foi interrompido devido a um erro.\")\n",
    "        return  # Interrompe o pipeline caso ocorra um erro\n",
    "\n",
    "    try:\n",
    "        # Extrair dados do arquivo CSV\n",
    "        extrai_csv.main()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro na extração de dados do arquivo CSV: {e}\")\n",
    "        print(\"O pipeline foi interrompido devido a um erro.\")\n",
    "        return  # Interrompe o pipeline caso ocorra um erro\n",
    "\n",
    "    try:\n",
    "        # Carregar dados no banco de dados\n",
    "        carrega_banco_postgres.carregar_dados_no_banco()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro no carregamento dos dados no banco de dados: {e}\")\n",
    "        print(\"O pipeline foi interrompido devido a um erro.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ffa05",
   "metadata": {},
   "source": [
    "## Consulta dos dados no banco\n",
    "* Após todas as etapas serem concluídas com sucesso, vamos rodar uma consulta para validar as informações de conexão entre as tabelas com o comando abaixo:\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "    o.order_id,\n",
    "    o.customer_id,\n",
    "    o.order_date,\n",
    "    od.product_id,\n",
    "    od.unit_price,\n",
    "    od.quantity,\n",
    "    od.discount\n",
    "FROM\n",
    "    datawarehouse.orders o\n",
    "JOIN\n",
    "    datawarehouse.order_details od ON o.order_id = od.order_id;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b36fc3",
   "metadata": {},
   "source": [
    "#### Autor\n",
    "Willian Ribeiro dos Santos\n",
    "\n",
    "Obrigado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32baad53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
